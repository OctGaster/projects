#Опис розв'язання задачі класифікації абонентів
##Опис даних
Файл tabular_data.csv містить числові дані щодо активності абонента за 12 періодів.<br>
• period – номер періоду (періоди послідовні, 1 – найновіший);<br>
• id – ідентифікатор абонента;<br>
• feature_0 – feature_49 – дані щодо активності абонента у відповідний період.<br>
Файл hashed_feature.csv – тут набір захешованих значень однієї категоріальної змінної для абонента.<br>
• id – ідентифікатор абонента;<br>
• feature_50 – хеш від значення категоріальної змінної.<br>
Файл train.csv – тут дані з цільовою міткою.<br>
• id – ідентифікатор абонента;<br>
• target – значення цільової мітки (1 – належить до сегменту водіїв, 0 – не належить до сегменту водіїв).<br>
Файл test.csv – список абонентів, для яких потрібно зробити передбачення.<br>
• id – ідентифікатор абонента;<br>
• score – ймовірність того, що абонент належить до сегменту водіїв (класу «1»). Цю ймовірність визначає навчена модель.<br>
##Яким чином оброблялися пропуски
    Усі змінні моделі були розділені на дві групи -- категоріальні та числові. Для категоріальних<br>
змінних пропуски заповнювалися модами відповідної змінної (значеннями, що найчастіше<br>
зустрічаються в наборі даних). Для числових змінних пропуски заповнювались медіанами<br>
розподілу відповідної змінної. У останньому випадку було вибрано саме медіану, а не<br>
середнє, аби зменшити чутливість вибірки до аномальних викидів, які в іншому разі могли<br>
б суттєво вплинути на значення для заповнення.<br>
##Яким чином оброблялися аномальні значення
    У вибірці присутні аномалії числових значень. Щоби зменшити їх вплив,<br>
до тренувальних даних було додано їх середні та стандартні відхилення, розраховані за<br>
12 періодів для кожного абонента окремо. Ці нові змінні враховувалися при класифікації<br>
додатково до вихідних фіч. Втім, після завершення конкурсу було виявлено, що<br>
повне відкидання старих змінних суттєво покращує якість моделі.<br>
##Як агрегували дані
    Тренувальні дані завантажувалися у два Pandas DataFrame'и -- tabular_data та<br>
hashed_feature. У tabular_data заповнювалися пропущені значення, та додавалися нові змінні -- середні<br>
та стандартні відхилення для кожної числової змінної кожного абонента окремо. Ці числа<br>
використовувалися далі для тренування класифікатора разом із вихідними<br>
(неагрегованими) значеннями змінних. Агрегування числових змінних дозволило збільшити<br>
площу під ROC на 0.4.<br>
    Для даних із hashed_feature було підраховано кількість унікальних захешованих значень<br>
для кожного абонента, і використано це число як нову змінну hashed_deriv. Такий спосіб<br>
агрегації було обрано, тому що це єдине числове значення, яке можна отримати із<br>
hashed_feature.csv. Кількість же унікальних значень feature_50 (5010) не дозволяє вважати її<br>
категоріальною змінною, попри умову задачі.<br>
    Було проаналізовано, як відрізняються середні значення усіх числових змінних між двома<br>
групами абонентів -- водіїв та не-водіїв. Виявилося, що у одиницях свого стандартного<br>
відхилення за групою не-водіїв hashed_deriv відрізняється найбільше з-поміж усіх інших<br>
числових змінних. На основі цього hashed_deriv було використано як змінну для класифікації.<br>
Це дало приріст площі під ROC на 0.1.<br>
    Також у ході попереднього аналізу було розраховано коефіцієнти кореляції усіх<br>
числових змінних з усіма іншими, та проведено усереднення цих коефіцієнтів за групами<br>
водіїв та не-водіїв. Виявилося, що не всі числові змінні є лінійно-незалежними. А саме,<br>
feature_31 та feature_34 демонструють сильну<br>
антикореляцію із значеннями періоду (модулі їх коефіцієнтів кореляції, усереднені за абонентами,<br>
більше 0.97). З цієї причини їх було вилучено із тренувального набору даних, як зайві.<br>
Інших лінійних кореляційних зв'язків між змінними моделі не виявлено.<br>
    Також з'ясувалося, що feature_41 має завжди нульові значення, отже теж не містить<br>
корисної інформації.<br>
##Як оброблялися категоріальні фічі
    Усі змінні набору даних було розділено на 2 групи -- категоріальні та числові. Розділ було<br>
проведено за кількістю унікальних значень відповідної змінної. Якщо ця кількість менша<br>
ніж деякий емпіричний ліміт (змінна categorical_limit у коді), змінна розглядається як<br>
насправді категоріальна. Добір ліміту було зроблено вручну.<br>
    Після розділення категоріальні змінні не бралися до аналізу, за браком часу.<br>
##Поділ dataset на train і test - який варіант обрали і чому
    Вихідний набір даних було розділено на тренувальну та тестову вибірки, шляхом<br>
випадкового вибору значень даних без повернення. Для цього було використано функцію<br>
train_test_split() із модуля sklearn.model_selection. Тестовий набір склав приблизно третину<br>
із усіх навчальних даних. Такий поділ було виконано для того, щоби запобігти<br>
надмірному навчанню моделі, забезпечивши її тестування на раніше небачених нею даних.<br>
train_test_split() було використано як готову реалізацію такого поділу, для<br>
прискорення розв'язання задачі.<br>
##Які алгоритми машинного навчання застосовували для розрахунку
    Для класифікації було застосовано випадковий ліс (random forest classifier), реалізований<br>
у модулі sklearn.ensemble. У лісі було 300 дерев, кожне глибиною до 4 вузлів. Було обрано<br>
саме випадковий ліс замість одного дерева, аби запобігти надмірному навчанню<br>
алгоритму.<br>
    Після завершення часу конкурсу алгоритм було покращено. А саме, ліс було розширено<br>
до 800 дерев, кожне глибиною до 10 вузлів. Завдяки цьому площа під ROC зросла до 0.99,<br>
а середня точність (average precision) стала 0.98.<br>
##Яким чином оцінювали якість моделей
    Класифікатор оцінювався за значенням площі під ROC (що ближча площа до 1,<br>
тим краще).<br>
##Яку модель / моделі обрали для фінального розрахунку і чому
    Для остаточної класифікації було використано випадковий ліс із 300 дерев, до 4 вузлів<br>
глибини кожне. Такий вибір обумовлений попередніми випробуваннями алгоритму, та<br>
браком часу для навчання більшої кількості складніших дерев.<br>
    Після завершення часу конкурсу було також досліджено, як на продуктивність моделі<br>
впливає включення похідних від фіч за часом (періодами). Під похідними маються на увазі<br>
різниці між значеннями змінних у двох послідовних періодах. Було вивчено вплив перших,<br>
других і третіх похідних, однак суттєвої зміни у площі під ROC не виявлено. Таким чином<br>
перевірялося, чи можна розділити класи абонентів за виглядом залежності їх фіч від часу,<br>
та застосувати модель аналізу часових послідовностей для класифікації.<br>
##Яким чином відбирали фічі, як змінився результат після відбору
    Для класифікації було відібрано тільки числові змінні, за браком часу для врахування<br>
категоріальних фіч.<br>
    Було розраховано коефіцієнти кореляції усіх числових змінних з усіма іншими, та<br>
усереднено ці коефіцієнти за групами<br>
водіїв та не-водіїв. Виявилося, що не всі числові змінні є лінійно-незалежними. А саме,<br>
feature_31 та feature_34 демонструють сильну<br>
антикореляцію із номером періоду (їх коефіцієнти кореляції, усереднені за абонентами,<br>
менше -0.97). З цієї причини їх було вилучено із тренувального набору даних, як зайві.<br>
Інших лінійних кореляційних зв'язків між змінними моделі не виявлено.<br>
Також з'ясувалося, що feature_41 має завжди нульові значення, отже теж не містить<br>
корисної інформації.<br>
##Які параметри тюнили в фінальній моделі, як покращився результат
    Кількість дерев, їх глибину та межу категоріальності змінної. Коли кількість дерев зросла<br>
зі 100 до 300, AUC на тестувальному датасеті не змінилася. Коли ж максимальна глибина<br>
дерева зросла із 3 до 4, AUC збільшилася із 0.77 до 0.79.<br>
##Який вийшов AUC на train і validation dataset у фінальній моделі
    0.8 на тренувальному датасеті та 0.79 на валідаційному.<br>
##Скільки фічей відібрали в фінальну модель
    139<br>
##Що хотіли зробити, але не встигли
    Планувалося зробити ліс із більшою кількістю складніших дерев, перевірити<br>
вплив похідних від фіч за періодами та провести ретельніший кореляційний аналіз<br>
змінних. Також хотілося порівняти продуктивність моделі випадкового лісу із нейронною мережею.
